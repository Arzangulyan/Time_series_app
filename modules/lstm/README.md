# Модуль LSTM для прогнозирования временных рядов

## Описание

Модуль `lstm` предоставляет инструменты для прогнозирования временных рядов с использованием нейронных сетей LSTM (Long Short-Term Memory). Модуль включает реализацию моделей LSTM, функции для подготовки данных, обучения моделей, оценки их качества и визуализации результатов.

## Математическое описание

LSTM (Long Short-Term Memory) - это тип рекуррентной нейронной сети, специально разработанный для эффективной обработки последовательностей данных. В отличие от обычных рекуррентных нейронных сетей, LSTM способен запоминать долговременные зависимости благодаря своей архитектуре с "ячейками памяти".

### Ключевые компоненты LSTM ячейки:

1. **Вентиль забывания (forget gate)** - определяет, какую информацию нужно удалить из состояния ячейки:
   $f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$

2. **Вентиль входа (input gate)** - определяет, какую новую информацию добавить в состояние ячейки:
   $i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)$
   $\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)$

3. **Обновление состояния ячейки**:
   $C_t = f_t * C_{t-1} + i_t * \tilde{C}_t$

4. **Вентиль выхода (output gate)** - определяет, какую информацию выдавать на выход:
   $o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)$
   $h_t = o_t * \tanh(C_t)$

где:
- $\sigma$ - сигмоидальная функция активации
- $\tanh$ - гиперболический тангенс
- $W_f, W_i, W_C, W_o$ - весовые матрицы
- $b_f, b_i, b_C, b_o$ - векторы смещения
- $h_{t-1}$ - выход с предыдущего шага
- $x_t$ - вход на текущем шаге
- $C_{t-1}$ - состояние ячейки на предыдущем шаге
- $C_t$ - состояние ячейки на текущем шаге
- $h_t$ - выход на текущем шаге

## Структура модуля

Модуль состоит из следующих файлов:

1. **models.py** - Содержит классы моделей:
   - `BaseTimeSeriesModel` - абстрактный базовый класс для моделей временных рядов
   - `LSTMModel` - модель LSTM для прогнозирования временных рядов

2. **core.py** - Основные функции для работы с LSTM:
   - `create_sequences` - создание последовательностей для обучения LSTM
   - `train_test_split_ts` - разделение временного ряда на обучающую и тестовую выборки
   - `auto_tune_lstm_params` - автоматический подбор параметров LSTM
   - `forecast_future` - прогнозирование будущих значений
   - `calculate_metrics` - расчет метрик качества
   - `prepare_data_for_forecast` - подготовка данных для прогнозирования

3. **visualization.py** - Функции для визуализации и диагностики:
   - `plot_time_series` - построение графика временного ряда
   - `plot_train_test_results` - визуализация результатов обучения и тестирования
   - `plot_forecast` - визуализация прогноза
   - `plot_training_history` - график процесса обучения
   - `plot_error_distribution` - анализ распределения ошибок
   - `display_model_information` - отображение информации о модели в Streamlit
   - `display_metrics` - отображение метрик качества в Streamlit

4. **utils.py** - Вспомогательные функции:
   - `check_input_series` - проверка и преобразование входных данных
   - `scale_time_series` - масштабирование временного ряда
   - `create_future_index` - создание индекса для будущих прогнозов
   - `generate_forecast_index` - генерация индекса для прогнозных значений
   - `save_results_to_csv` - сохранение результатов в CSV
   - `load_model_from_file` - загрузка модели из файла

## Как использовать модуль

### Импорт необходимых компонентов

```python
from modules.lstm import (
    LSTMModel, 
    create_sequences, 
    train_test_split_ts, 
    auto_tune_lstm_params, 
    calculate_metrics,
    plot_forecast,
    plot_training_history
)
```

### Подготовка данных и подбор параметров

```python
import pandas as pd

# Загрузка данных
data = pd.read_csv('timeseries_data.csv', parse_dates=['date'], index_col='date')
time_series = data['value']

# Автоматический подбор параметров
params = auto_tune_lstm_params(time_series)
print(f"Рекомендуемые параметры LSTM: {params}")

# Разделение на обучающую и тестовую выборки
train_size = 0.8
train_data, test_data = train_test_split_ts(time_series, train_size)
```

### Создание и обучение модели

```python
# Создание модели с выбранными параметрами
model = LSTMModel(
    sequence_length=params['sequence_length'],
    units=params['units'],
    dropout_rate=params['dropout_rate'],
    bidirectional=params['bidirectional']
)

# Обучение модели
trained_model = model.fit(
    series=train_data,
    epochs=params['epochs'],
    batch_size=params['batch_size'],
    validation_split=params['validation_split'],
    early_stopping=params['early_stopping'],
    patience=params['patience'],
    verbose=1
)

# Визуализация процесса обучения
history_plot = plot_training_history(model.training_history)
```

### Оценка качества модели

```python
# Прогнозирование на тестовой выборке
test_predictions = model.predict(steps=len(test_data))

# Расчет метрик качества
metrics = calculate_metrics(test_data.values, test_predictions.values)
print(f"RMSE: {metrics['rmse']:.4f}")
print(f"MAE: {metrics['mae']:.4f}")
print(f"MAPE: {metrics['mape']:.2f}%")
```

### Прогнозирование будущих значений

```python
# Прогноз на 30 шагов вперед
forecast = model.predict(steps=30)

# Визуализация прогноза
forecast_plot = plot_forecast(time_series, forecast)

# Сохранение модели для последующего использования
model.save_model('models/lstm_model')
```

### Интеграция с Streamlit

```python
import streamlit as st
from modules.lstm.visualization import display_model_information, display_metrics

# Отображение информации о модели
display_model_information(model.get_params())

# Отображение метрик качества
display_metrics(metrics)

# Отображение графика прогноза
st.plotly_chart(forecast_plot)
```

## Требования к данным

Для оптимальной работы LSTM моделей рекомендуется:

1. Использовать временные ряды с не менее чем 100 наблюдениями
2. Предварительно обрабатывать данные (удалять выбросы, заполнять пропуски)
3. Масштабировать данные перед подачей в модель (функция `scale_time_series`)
4. Выбирать подходящую длину последовательности (sequence_length) в зависимости от характеристик ряда

## Ограничения

- LSTM требует значительных вычислительных ресурсов для обучения на больших объемах данных
- Для рядов с сильной периодичностью может потребоваться дополнительная предобработка
- Качество прогноза снижается при увеличении горизонта прогнозирования

## Дополнительная информация

LSTM модели особенно эффективны для:
- Временных рядов с долговременными зависимостями
- Данных с сложной структурой, которую сложно моделировать статистическими методами
- Рядов с нелинейными взаимосвязями

Для улучшения результатов рекомендуется:
- Экспериментировать с различными архитектурами (изменять количество слоев и нейронов)
- Использовать раннюю остановку для предотвращения переобучения
- Комбинировать прогнозы нескольких моделей для повышения точности

# Документация по уровням сложности (complexity) LSTM-модели

В модуле реализована функция `auto_tune_lstm_params`, которая позволяет быстро выбрать архитектуру LSTM-модели на основе уровня сложности (`complexity_level`). Доступны три предустановленных подхода:

## 1. Simple (Низкая сложность)

- **Архитектура:**
  - Один LSTM-слой
  - 64 нейрона
  - Dropout: 0.1
  - Bidirectional: Нет
- **Параметры обучения:**
  - batch_size: 16
  - epochs: 50 (по умолчанию, можно переопределить)
  - sequence_length: 5 (по умолчанию, можно переопределить)
- **Рекомендации:**
  - Для коротких рядов (менее 500 точек)
  - Для простых паттернов, когда нет выраженной сезонности или сложных зависимостей
  - Быстрое прототипирование

## 2. Medium (Средняя сложность)

- **Архитектура:**
  - Два LSTM-слоя
  - 128 и 64 нейрона
  - Dropout: 0.2
  - Bidirectional: Нет
- **Параметры обучения:**
  - batch_size: 32
  - epochs: 100 (по умолчанию, можно переопределить)
  - sequence_length: 7 (по умолчанию, можно переопределить)
- **Рекомендации:**
  - Для рядов средней длины (500–2000 точек)
  - Для данных с умеренной сложностью и возможной сезонностью
  - Баланс между скоростью и качеством

## 3. Complex (Высокая сложность)

- **Архитектура:**
  - Три LSTM-слоя
  - 256, 128 и 64 нейрона
  - Dropout: 0.3
  - Bidirectional: Да
- **Параметры обучения:**
  - batch_size: 64
  - epochs: 150 (по умолчанию, можно переопределить)
  - sequence_length: 10 (по умолчанию, можно переопределить)
- **Рекомендации:**
  - Для длинных и сложных рядов (2000+ точек)
  - Для задач с выраженной сезонностью, нелинейными зависимостями
  - Когда требуется максимальное качество прогноза и есть ресурсы для долгого обучения

---

## Как выбрать уровень сложности?

- **Simple:** если данных мало, задача простая, нужен быстрый результат.
- **Medium:** если данных достаточно, задача средней сложности, требуется баланс между скоростью и качеством.
- **Complex:** если данных много, задача сложная, важна точность прогноза.

## Важно
- Все параметры (sequence_length, epochs и др.) можно переопределить вручную после выбора шаблона.
- Если требуется полный контроль — задавайте параметры вручную, минуя шаблоны.

## Пример использования

```python
params = auto_tune_lstm_params(time_series, complexity_level='medium')
params['sequence_length'] = 12  # Переопределяем при необходимости
params['epochs'] = 80
```

---

**Реализация шаблонов находится в функции `auto_tune_lstm_params` файла `core.py`.**

# Описание параметров, задаваемых в интерфейсе Streamlit (pages/5_LSTM.py)

В интерфейсе Streamlit для настройки и обучения LSTM-модели доступны следующие параметры:

| Параметр                | Описание                                                                 | Диапазон/Тип         | Влияние на модель                                  |
|------------------------ |-----------------------------------------------------------------------|----------------------|----------------------------------------------------|
| **Размер обучающей выборки** (`train_size`) | Доля данных, используемых для обучения (остальное — для теста) | 0.5 – 0.95 (float)   | Чем больше, тем больше данных для обучения, но меньше для проверки качества. |
| **Длина входной последовательности** (`sequence_length`) | Сколько предыдущих точек используется для предсказания следующей | 1 – 30 (int)         | Чем больше, тем больше информации о прошлом, но сложнее модель. |
| **Сложность модели** (`model_complexity`)   | Выбор шаблона архитектуры (см. раздел выше)                     | 'Низкая', 'Средняя', 'Высокая' | Определяет количество слоёв, нейронов, dropout и bidirectional. |
| **Количество эпох** (`epochs`)              | Сколько раз модель проходит по обучающей выборке                 | 10 – 200 (int)       | Больше эпох — выше шанс дообучиться, но и переобучиться. |
| **Шаги прогноза вперёд** (`forecast_steps`) | На сколько шагов вперёд делать прогноз                           | 0 – 100 (int)        | Определяет горизонт прогноза. Чем больше, тем выше неопределённость. |

### Дополнительные параметры, задаваемые автоматически или через шаблон:

| Параметр                | Описание                                                                 | Значение по умолчанию/шаблон | Влияние |
|------------------------ |-----------------------------------------------------------------------|------------------------------|---------|
| **units**               | Количество нейронов в каждом LSTM-слое                                 | См. complexity               | Чем больше, тем выше потенциальная точность, но и риск переобучения. |
| **dropout_rate**        | Доля нейронов, отключаемых при обучении для регуляризации               | См. complexity               | Помогает бороться с переобучением. |
| **bidirectional**       | Используется ли двунаправленный LSTM                                   | См. complexity               | Позволяет учитывать информацию и из будущего, но увеличивает сложность. |
| **batch_size**          | Размер мини-батча при обучении                                         | См. complexity               | Влияет на скорость и стабильность обучения. |
| **validation_split**    | Доля обучающей выборки для валидации                                   | 0.1                          | Используется для ранней остановки и контроля переобучения. |
| **early_stopping**      | Использовать ли раннюю остановку                                       | True                         | Останавливает обучение, если качество не улучшается. |
| **patience**            | Сколько эпох ждать улучшения перед остановкой                          | 15                           | Чем больше, тем дольше ждать улучшения. |

---

## Рекомендации по выбору параметров

- **train_size**: Обычно 0.7–0.9. Если данных мало — лучше оставить больше для теста.
- **sequence_length**: Для рядов с сезонностью — кратно периоду, для обычных — 5–15.
- **epochs**: Для быстрой проверки — 20–50, для финального обучения — 100+ (с ранней остановкой).
- **forecast_steps**: Зависит от задачи. Для краткосрочного прогноза — 5–20, для долгосрочного — больше.
- **model_complexity**: См. раздел выше. Если не уверены — начните со 'Средней'.

---

**Все параметры можно гибко настраивать под задачу. Для полного контроля архитектуры используйте ручной ввод параметров, минуя шаблоны сложности.**